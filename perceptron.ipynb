{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Multiclass Kernel Perceptrons\n",
    "IDs: 23225718 and <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassKernelPerceptron():\n",
    "    \"\"\"\n",
    "    A multiclass implementation of the kernel perceptron\n",
    "    Designed for the MNIST dataset to classify digits 0-9\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, kernel, num_digits=10):\n",
    "        \"\"\"\n",
    "        Instantiates the perceptron instance\n",
    "\n",
    "        Args:\n",
    "            kernel: kernel function to use\n",
    "            num_digits: optional param- number of digits to classify,\n",
    "                assumes digits range from 0...num_digits\n",
    "        \"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.num_classes = num_digits\n",
    "\n",
    "    def train(self, X, Y, kernel_matrix=None, epochs=10):   \n",
    "        \"\"\"\n",
    "        Trains the perceptron\n",
    "\n",
    "        Args:\n",
    "            X: training data (n x d)\n",
    "            Y: training labels (n x 1)\n",
    "            kernel_matrix: optional pre-calculated kernel matrix\n",
    "            epochs: number of epochs to run\n",
    "        \"\"\"\n",
    "        self.X_training = X\n",
    "        Y = Y.astype(int)\n",
    "\n",
    "        # Calculate gram matrix if necessary\n",
    "        if kernel_matrix is not None:\n",
    "            self.gram_matrix = kernel_matrix\n",
    "        else: \n",
    "            self.gram_matrix = self.get_kernel_matrix(self.kernel, X)\n",
    "\n",
    "        self.alpha = self.update_alpha(self.gram_matrix, Y, self.num_classes, epochs)\n",
    "    \n",
    "    # Pulling this into a separate function for numba,\n",
    "    #   but consider this as if it were appended to the train method\n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def update_alpha(gram_matrix, Y, num_classes, epochs):\n",
    "        \n",
    "        num_data = gram_matrix.shape[0]\n",
    "        alpha = np.zeros((num_data, num_classes), dtype=np.float32)\n",
    "\n",
    "        # To boost efficiency, let's keep track of non-zero alpha weights\n",
    "        # Note that a data index is nontrivial if its alpha is nonzero for ANY perceptron\n",
    "        nontrivial_indices = np.empty(num_data, dtype=np.int32)\n",
    "        cur_nontrivial_idx = 0\n",
    "\n",
    "        # Instantiate once and reuse in update step\n",
    "        label_vector = -1 * np.ones(num_classes, dtype=np.float32)\n",
    "        zeros = np.zeros(num_classes, dtype=np.float32)\n",
    "        decisions = np.sign(zeros)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_data):\n",
    "                # Get decisions for all the classifiers\n",
    "                if (cur_nontrivial_idx > 0):\n",
    "                    # Pull out the relevant values of alpha and the gram matrix\n",
    "                    data_to_consider = nontrivial_indices[:cur_nontrivial_idx]\n",
    "                    decisions = np.sign(np.dot(\n",
    "                        alpha[data_to_consider].T,\n",
    "                        gram_matrix[:, i][data_to_consider]\n",
    "                    ))\n",
    "\n",
    "                # The label vector should be -1 for every index\n",
    "                # except the one corresponding to the current Yi label\n",
    "                label_vector[Y[i]] = 1\n",
    "\n",
    "                # For a given classifier, if the decision * label <= 0,\n",
    "                # the update is the label, otherwise it is zero\n",
    "                update = np.where(decisions*label_vector <=0, label_vector, zeros)\n",
    "\n",
    "                # Add the update to alpha, reset label vector\n",
    "                if (np.any(update != 0) and not np.any(alpha[i,:] != 0)):\n",
    "                    nontrivial_indices[cur_nontrivial_idx] = i\n",
    "                    cur_nontrivial_idx += 1\n",
    "\n",
    "                alpha[i,:] += update\n",
    "                label_vector[Y[i]] = -1\n",
    "        \n",
    "        return alpha\n",
    "                \n",
    "    def predict(self, X, kernel_matrix=None):\n",
    "        \"\"\"\n",
    "        Predicts the labels of a new set of data\n",
    "\n",
    "        Args:\n",
    "            X: data to classify\n",
    "            kernel_matrix: an optional parameter for a precalculated kernel\n",
    "                between X and the training data\n",
    "        \"\"\"\n",
    "        # If we need to compute a kernel matrix, we only need to find distances\n",
    "        #   where the alphas are not 0.\n",
    "        relevant_indices = np.where(np.sum(self.alpha != 0, axis=1) > 0)[0]\n",
    "        if kernel_matrix is None:\n",
    "            kernel_matrix = self.get_kernel_matrix(self.kernel, X, self.X_training[relevant_indices])\n",
    "        else: \n",
    "            kernel_matrix = kernel_matrix[:, :][:, relevant_indices]\n",
    "        \n",
    "        alpha = self.alpha[relevant_indices]\n",
    "\n",
    "        # See how confident each classifier is that a given point is in that class\n",
    "        #   i.e. the distance between the point and the decision boundary\n",
    "        # We want to get a N x K matrix where N = number of classifiers, K = num data in X\n",
    "        # Each value is given by the dot product of the alphas for the classifier\n",
    "        #   and the distances from the data point to the training examples\n",
    "        data_confidence_by_classifier = kernel_matrix @ alpha\n",
    "        \n",
    "        # Pick the label of the classifier with the max confidence\n",
    "        # Each classifier operates on a 1-v-All basis\n",
    "        predictions = np.argmax(data_confidence_by_classifier, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def get_kernel_matrix(kernel_func, X1, X2=None):\n",
    "        \"\"\"\n",
    "        Calculates the full kernel matrix between X1 and X2\n",
    "        using an input kernel function\n",
    "\n",
    "        Args:\n",
    "            kernel_func: kernel function\n",
    "            X1: Ax1 vector\n",
    "            X2: optional Bx1 vector\n",
    "        Returns:\n",
    "            (AxB) matrix of kernel distances if X2 is given\n",
    "            (AxA) matrix otherwise\n",
    "        \"\"\"\n",
    "        A = X1.shape[0]\n",
    "        if X2 is None:\n",
    "            # In one-matrix case, we can use symmetry\n",
    "            kernel_matrix = np.empty((A,A), dtype=np.float32)\n",
    "            for i in range(A):\n",
    "                for j in range(i,A):\n",
    "                    kernel_matrix[i, j] = kernel_func(X1[i], X1[j])\n",
    "                    kernel_matrix[j, i] = kernel_matrix[i, j]\n",
    "        else:\n",
    "            B = X2.shape[0]\n",
    "            kernel_matrix = np.empty((A,B), dtype=np.float32)\n",
    "            for i in range(A):\n",
    "                for j in range(B):\n",
    "                    kernel_matrix[i, j] = kernel_func(X1[i], X2[j])\n",
    "        return kernel_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_fraction):\n",
    "    \"\"\"\n",
    "    Splits the data into two sets with the specified fraction.\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "        split_fraction: fraction to split\n",
    "    Returns:\n",
    "        larger dataset, smaller dataset, shuffled indices, and split index\n",
    "    \"\"\"\n",
    "    num_data = data.shape[0]\n",
    "    indices = np.arange(num_data)\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data = data[indices]\n",
    "\n",
    "    split_index = int(num_data * split_fraction)\n",
    "    return shuffled_data[:split_index], shuffled_data[split_index:], indices, split_index\n",
    "\n",
    "def get_cross_validation_indices(data, num_folds):\n",
    "    \"\"\"\n",
    "    Gets the indices of the data that represent a cross-validation split\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "        num_folds: number of folds\n",
    "    Returns:\n",
    "        list of (start, end) tuples\n",
    "    \"\"\"\n",
    "    num_data = data.shape[0]\n",
    "    data_per_fold = int(num_data / num_folds)\n",
    "\n",
    "    start = 0\n",
    "    index_tuples = []\n",
    "    for _ in range(num_folds-1):\n",
    "        index_tuples.append((start, start + data_per_fold))\n",
    "        start += data_per_fold\n",
    "    \n",
    "    index_tuples.append((start, num_data))\n",
    "    return index_tuples\n",
    "\n",
    "def get_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Splits the data from the labels\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "    Returns:\n",
    "        data, labels\n",
    "    \"\"\"\n",
    "    return data[:,1:], data[:,0]\n",
    "\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    \"\"\"\n",
    "    Returns predicted accuracy\n",
    "\n",
    "    Args:\n",
    "        predictions: numpy array of predictions\n",
    "        actual: actual values\n",
    "    Returns:\n",
    "        accuracy float\n",
    "    \"\"\"\n",
    "    num_correct = np.sum(predictions == actual)\n",
    "    return num_correct / actual.shape[0]\n",
    "\n",
    "def get_polynomial_kernel(degree):\n",
    "    \"\"\"\n",
    "    Returns a polynomial kernel with specified degree\n",
    "\n",
    "    Args:\n",
    "        degree: degree of polynomial\n",
    "    Returns:\n",
    "        polynomial kernel function\n",
    "    \"\"\"\n",
    "    @jit(nopython=True)\n",
    "    def polynomial_kernel(x1, x2):\n",
    "        return (np.dot(x1, x2)) ** degree\n",
    "    \n",
    "    return polynomial_kernel\n",
    "\n",
    "def split_kernels(kernel, split_index):\n",
    "    \"\"\"\n",
    "    Returns a training and test kernel from an index to split\n",
    "\n",
    "    Args:\n",
    "        kernel: NxN gram matrix\n",
    "        split_index: index < N, number of training examples\n",
    "    Returns:\n",
    "        training kernel matrix (square), test kernel matrix (rectangle)\n",
    "    \"\"\"\n",
    "    # Grab the first split_index elements for the square training kernel\n",
    "    #   and one of the remaining rectangles for the test kernel \n",
    "    training = kernel[:split_index,:split_index]\n",
    "    test = kernel[split_index:,:split_index]\n",
    "    return training, test\n",
    "\n",
    "def get_training_validation_kernels(kernel, start, end):\n",
    "    \"\"\"\n",
    "    Returns a training and validation kernel from cross-validation indices\n",
    "    The start-end defines the validation region\n",
    "\n",
    "    Args:\n",
    "        kernel: NxN gram matrix\n",
    "        start, end: indices < N of the validation region\n",
    "    Returns:\n",
    "        training kernel matrix (square), validation kernel matrix (rectangle)\n",
    "    \"\"\"\n",
    "    # Build training array by picturing four corners of a square\n",
    "    left = np.vstack((kernel[:start,:start], kernel[end:,:start]))\n",
    "    right = np.vstack((kernel[:start,end:], kernel[end:,end:]))\n",
    "    training = np.hstack((left, right))\n",
    "\n",
    "    # Build validation array by picturing a rectangle\n",
    "    valid = np.hstack((kernel[start:end, :start], kernel[start:end, end:]))\n",
    "    return training, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Basic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "data = np.genfromtxt('./zipcombo.dat')\n",
    "\n",
    "# Define polynomial perceptrons, pre-calculate kernels\n",
    "poly_params = [d+1 for d in range(7)]\n",
    "poly_perceptrons = [MultiClassKernelPerceptron(get_polynomial_kernel(p)) for p in poly_params]\n",
    "poly_kernels = [p.get_kernel_matrix(p.kernel, data) for p in poly_perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "Currently running iter 1\n",
      "Currently running iter 2\n",
      "Currently running iter 3\n",
      "Currently running iter 4\n"
     ]
    }
   ],
   "source": [
    "def run_20_iters(perceptrons, kernels, data):\n",
    "    \"\"\"\n",
    "    Follows the routine in Q1 of the coursework: \n",
    "        Reports the best accuracy of the perceptrons over 20 iterations\n",
    "\n",
    "    Args:\n",
    "        perceptrons: perceptrons to use\n",
    "        kernels: pre-calculated kernels corresponding to the perceptrons\n",
    "        data: the data\n",
    "    Returns:\n",
    "        accuracy stats for the 20 runs for each perceptron\n",
    "    \"\"\"\n",
    "        \n",
    "    stats = [[] for _ in range(len(perceptrons))]\n",
    "    for iter in range(20):\n",
    "        print(\"Currently running iter\", iter)\n",
    "        training, test, shuffle_indices, split_index = split_data(data, 0.8)\n",
    "        train_X, train_Y = get_data_and_labels(training)\n",
    "        test_X, test_Y = get_data_and_labels(test)\n",
    "\n",
    "        for i, perceptron in enumerate(perceptrons):\n",
    "            # print(\"> Perceptron\", i)\n",
    "            shuffled_kernel = kernels[i][shuffle_indices, :][:, shuffle_indices]\n",
    "            training_kernel, test_kernel = split_kernels(shuffled_kernel, split_index)\n",
    "\n",
    "            perceptron.train(train_X, train_Y, training_kernel)\n",
    "            predictions = perceptron.predict(test_X, test_kernel)\n",
    "            accuracy = calculate_accuracy(predictions, test_Y)\n",
    "            stats[i].append(accuracy)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "q1_stats = run_20_iters(poly_perceptrons, poly_kernels, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy by kernel degree\n",
      "1 - 0.9563, 0.9680\n",
      "2 - 0.9789, 0.9847\n",
      "3 - 0.9825, 0.9898\n",
      "4 - 0.9840, 0.9904\n",
      "5 - 0.9827, 0.9891\n",
      "6 - 0.9817, 0.9865\n",
      "7 - 0.9802, 0.9878\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics!\n",
    "def get_confidence_range(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return [mean-std, mean+std]\n",
    "\n",
    "def prettify_stat(arr):\n",
    "    return \", \".join([\"{:.4f}\".format(elm) for elm in arr])\n",
    "\n",
    "q1_summary = [get_confidence_range(runs) for runs in q1_stats]\n",
    "\n",
    "print(\"Test accuracy by kernel degree\")\n",
    "for i, qsum in enumerate(q1_summary):\n",
    "    print(poly_params[i], \"-\", prettify_stat(qsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "Currently running iter 1\n",
      "Currently running iter 2\n",
      "Currently running iter 3\n",
      "Currently running iter 4\n",
      "Currently running iter 5\n",
      "Currently running iter 6\n",
      "Currently running iter 7\n",
      "Currently running iter 8\n",
      "Currently running iter 9\n",
      "Currently running iter 10\n",
      "Currently running iter 11\n",
      "Currently running iter 12\n",
      "Currently running iter 13\n",
      "Currently running iter 14\n",
      "Currently running iter 15\n",
      "Currently running iter 16\n",
      "Currently running iter 17\n",
      "Currently running iter 18\n",
      "Currently running iter 19\n"
     ]
    }
   ],
   "source": [
    "def run_cv_20_iters(perceptrons, kernels, data, params, confusion_stats=None):\n",
    "    \"\"\"\n",
    "    Follows the routine in Q2 of the coursework: \n",
    "        Performs 5-fold CV to find the optimal hyperparameter\n",
    "        Optionally outputs confusion stats\n",
    "\n",
    "    Args:\n",
    "        perceptrons: perceptrons to use\n",
    "        kernels: pre-calculated kernels corresponding to the perceptrons\n",
    "        params: the parameters that are cross-validated over\n",
    "        data: the data to use\n",
    "        confusion_stats: optional array to store confusion stats\n",
    "    Returns:\n",
    "        average accuracy, degree for the different runs\n",
    "    \"\"\"\n",
    "    accuracy_stats = []\n",
    "    best_params = []\n",
    "\n",
    "    for iter in range(20):\n",
    "        print(\"Currently running iter\", iter)\n",
    "        training, test, shuffle_indices, split_index = split_data(data, 0.8)\n",
    "        test_X, test_Y = get_data_and_labels(test)\n",
    "        full_train_X, full_train_Y = get_data_and_labels(training)\n",
    "\n",
    "        # Cross-Validation step\n",
    "        num_folds = 5\n",
    "        cv_indices = get_cross_validation_indices(training, num_folds)\n",
    "\n",
    "        # Pre-calculate kernel matrices\n",
    "        shuffled_kernels = [split_kernels(k[shuffle_indices, :][:, shuffle_indices], split_index) for k in kernels]\n",
    "        cv_accuracies = [[] for _ in perceptrons]\n",
    "        \n",
    "        # Iterate through CV sets\n",
    "        for cur_idx in cv_indices:\n",
    "            # print(\"> Cross-validation\", cur_idx)\n",
    "            start, end = cur_idx\n",
    "            cur_valid = training[start:end,:]\n",
    "            cur_train = np.concatenate((training[:start,:], training[end:,:]))\n",
    "\n",
    "            train_X, train_Y = get_data_and_labels(cur_train)\n",
    "            valid_X, valid_Y = get_data_and_labels(cur_valid)\n",
    "\n",
    "            # For each set, evaluate each perceptron\n",
    "            for i, perceptron in enumerate(perceptrons):\n",
    "                # print(\">> Perceptron\", i)\n",
    "                # Grab the training kernel of the i'th shuffled kernel\n",
    "                full_training_kernel = shuffled_kernels[i][0]\n",
    "                training_kernel, validation_kernel = get_training_validation_kernels(full_training_kernel, start, end)\n",
    "                perceptron.train(train_X, train_Y, training_kernel)\n",
    "                predictions = perceptron.predict(valid_X, validation_kernel)\n",
    "\n",
    "                accuracy = calculate_accuracy(predictions, valid_Y)\n",
    "                cv_accuracies[i].append(accuracy)\n",
    "        \n",
    "        # Choose the best perceptron, use it to train the whole set\n",
    "        best_perceptron = np.argmax(np.mean(cv_accuracies, axis=1))\n",
    "        best_params.append(params[best_perceptron])\n",
    "\n",
    "        # Leverage pre-computed kernel matrix\n",
    "        training_kernel, test_kernel = shuffled_kernels[best_perceptron]\n",
    "\n",
    "        chosen_perceptron = perceptrons[best_perceptron]\n",
    "        chosen_perceptron.train(full_train_X,full_train_Y,training_kernel)\n",
    "        test_predictions = chosen_perceptron.predict(test_X, test_kernel)\n",
    "        test_accuracy = calculate_accuracy(test_predictions, test_Y)\n",
    "        accuracy_stats.append(test_accuracy)\n",
    "\n",
    "        if confusion_stats is not None:\n",
    "            confusion_stats.append((test_X, test_Y, test_predictions))\n",
    "\n",
    "    return accuracy_stats, best_params\n",
    "\n",
    "q3_confusion_stats = []\n",
    "q2_stats, q2_ds = run_cv_20_iters(poly_perceptrons, poly_kernels, data, poly_params, q3_confusion_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy\n",
      "0.9835, 0.9884\n",
      "Cross-validated degrees\n",
      "3.0091, 5.1909\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics!\n",
    "q2_summary_accuracies = get_confidence_range(q2_stats)\n",
    "q2_summary_degrees = get_confidence_range(q2_ds)\n",
    "\n",
    "print(\"Cross-validated accuracy\")\n",
    "print(prettify_stat(q2_summary_accuracies))\n",
    "print(\"Cross-validated degrees\")\n",
    "print(prettify_stat(q2_summary_degrees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Confusion Matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "0.0000, 0.0011, 0.0017, 0.0005, 0.0010, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000\n",
      "0.0002, 0.0000, 0.0008, 0.0002, 0.0019, 0.0000, 0.0004, 0.0000, 0.0000, 0.0000\n",
      "0.0035, 0.0021, 0.0000, 0.0082, 0.0048, 0.0002, 0.0005, 0.0019, 0.0003, 0.0000\n",
      "0.0021, 0.0009, 0.0074, 0.0000, 0.0027, 0.0116, 0.0000, 0.0023, 0.0015, 0.0000\n",
      "0.0003, 0.0021, 0.0018, 0.0003, 0.0000, 0.0021, 0.0065, 0.0017, 0.0006, 0.0003\n",
      "0.0011, 0.0003, 0.0014, 0.0088, 0.0048, 0.0000, 0.0048, 0.0011, 0.0024, 0.0000\n",
      "0.0003, 0.0000, 0.0003, 0.0000, 0.0043, 0.0024, 0.0000, 0.0000, 0.0042, 0.0000\n",
      "0.0003, 0.0000, 0.0003, 0.0012, 0.0044, 0.0013, 0.0000, 0.0000, 0.0062, 0.0058\n",
      "0.0000, 0.0003, 0.0000, 0.0000, 0.0011, 0.0037, 0.0021, 0.0042, 0.0000, 0.0099\n",
      "0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0020, 0.0027, 0.0000\n"
     ]
    }
   ],
   "source": [
    "def calc_confusion_stats(confusion_stat):\n",
    "    test_X, test_Y, preds = confusion_stat\n",
    "\n",
    "    digits, counts = np.unique(test_Y, return_counts=True)\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "\n",
    "    for i, y in enumerate(test_Y.astype(int)):\n",
    "        if (y != preds[i]):\n",
    "            confusion_matrix[y, preds[i]] += 1\n",
    "    \n",
    "    # Divide each row by count of that digit\n",
    "    normalized_confusion_matrix = confusion_matrix / counts.reshape(-1, 1)\n",
    "    return normalized_confusion_matrix\n",
    "\n",
    "test_q3_stats = np.array([calc_confusion_stats(stat) for stat in q3_confusion_stats])\n",
    "mean_confusion_matrix = np.mean(test_q3_stats, axis=0)\n",
    "\n",
    "pretty_confusion = [prettify_stat(row) for row in mean_confusion_matrix]\n",
    "print(\"Confusion Matrix:\")\n",
    "for confusion_row in pretty_confusion: print(confusion_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Mistaken digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3, Actual: 2. Wrong 6 times\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 3, Actual: 5. Wrong 6 times\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEn0lEQVR4nO3dQSusbRyA8Rm9UWchIqVYKIkvwJaFndhZ+ABWsuADjGbjA2A1sbGasqAUSwtFspZCZDZmLIaNRPHuzunt7e35v2eGea4512/9737uTte5F+6eebKfn5+fGQmmpdEbkH6H4QrJcIVkuEIyXCEZrpAMV0iGKyTDFdJf0cFsNvuV+/htMzMzoblCoRCay+VyiTOlUim01vn5eWju9fU1cebp6Sm0Fl30ItcTV0iGKyTDFZLhCslwhWS4QjJcIRmukAxXSNnoO2dpvTm7uLgIzY2MjHzxTv7t5eUlNPf4+Jg4s7W1FVorn8+H5tLKmzM1NcMVkuEKyXCFZLhCMlwhGa6QDFdIqb6AiDzz9PQ0tNbo6Git22mo29vb0Nzg4GDizMfHR63b+TJeQKipGa6QDFdIhiskwxWS4QrJcIVkuEIyXCGFf/SuESK3KMPDw9+wk8bb3t4OzaX5VqyePHGFZLhCMlwhGa6QDFdIhiskwxWS4Qop1RcQEUdHR6G56enp0Nz19XXizOLiYmit8fHx0Fzky0HVajW01p/CE1dIhiskwxWS4QrJcIVkuEIyXCEZrpAMV0ip/tG7iOiN2N7eXmiuXC4nziwvL4fW6u7uDs11dHQkzgwNDYXWOjk5SZxZX18PrdUI/uidmprhCslwhWS4QjJcIRmukAxXSIYrJMMVEv7mbHZ2NjRXLBa/eCfp8PDwkDgzNjYWWuv+/r7W7fxv3pypqRmukAxXSIYrJMMVkuEKyXCFZLhCwl9A9PT0hOYuLy9Dc52dnbVs5x+if0x/e3tLnGlra6t1Oz/Nz8+H5gqFQt2eGeUFhJqa4QrJcIVkuEIyXCEZrpAMV0iGKyTDFRL+c1GVSiU0d3h4GJrr7e1NnLm7uwutFd3b2dlZ4szOzk5orYjIJ7HSzhNXSIYrJMMVkuEKyXCFZLhCMlwhGa6QDFdI+JuzqLm5uUZv4T9NTEzUba1qtZo4c3NzU7fnNYonrpAMV0iGKyTDFZLhCslwhWS4QjJcIf0xFxCN0NraGprb3Nys2zPz+XziTKlUqtvzGsUTV0iGKyTDFZLhCslwhWS4QjJcIRmukAxXSKm+OWtvb0+cWVpaCq1VLpdDc8ViMXEm+umsjY2N0NzAwEBoLqKrqytxJvpJpjTzxBWS4QrJcIVkuEIyXCEZrpAMV0iGK6TsZ/Cv0dE/utfTwsJC4sza2lpdnxl5rSX6b9HX11frdn46Pj4OzU1NTSXOPD8/17qdLxO9HPHEFZLhCslwhWS4QjJcIRmukAxXSIYrJMMVUqpf3bm6uvr2Z/b393/7M1dWVhJnIj9ml8k0x2s5EZ64QjJcIRmukAxXSIYrJMMVkuEKyXCFZLhCSvU7Zz9+/Eic2d3dDa01OTlZ425+eX9/D83lcrnQ3Orqai3baSq+c6amZrhCMlwhGa6QDFdIhiskwxWS4Qop1RcQES0tsf97ka/pZDKZTKVSSZw5ODgIrbW/vx+a0y9eQKipGa6QDFdIhiskwxWS4QrJcIVkuEIyXCGFb86kNPHEFZLhCslwhWS4QjJcIRmukAxXSIYrJMMV0t9qZ+5juPzBGwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 8, Actual: 7. Wrong 6 times\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEIUlEQVR4nO3csUpcaxiF4TNHkUC00SplhBFTCIKFabVKIdgKgmlSSPQ6vAY7QbQQO0shZVotFNRGsIyBYCFoUJlcwv6OZnDWnOepF78bfNmFP9tWp9Pp/ANh/n3tB4DnEC6RhEsk4RJJuEQSLpGESyThEkm4RBqsDlutVjef49kmJydLu7W1tdLu9PS0cfPp06fSWYuLi6XdyspK42ZnZ6d0VrrqRa43LpGESyThEkm4RBIukYRLJOESSbhEEi6RWtVvzl7j5qzyM8/Pz0tnTUxMlHa3t7eNm+Hh4dJZVZXbuqmpqb/6M3uVmzP6mnCJJFwiCZdIwiWScIkkXCIJl0jlT3deQ+WP0Zubm6Wzqp/bbG1tNW7u7+9LZ83NzZV2q6urjZvR0dHSWb9+/Srt0nnjEkm4RBIukYRLJOESSbhEEi6RhEsk4RKppz/dSTc9PV3aHR8fN24+f/5cOmt7e7u061U+3aGvCZdIwiWScIkkXCIJl0jCJZJwidTTn+6ku7i4KO2enp4aNzMzM6Wz0i8gqrxxiSRcIgmXSMIlknCJJFwiCZdIwiWScInk5qyL7u7uSrvr6+vGTbvdfunj9BVvXCIJl0jCJZJwiSRcIgmXSMIlknCJJFwiuTnroqGhodJuYGCgcTMyMvLSx+kr3rhEEi6RhEsk4RJJuEQSLpGESyThEskFRBc9PDyUdm/evGnc/P79+6WP01e8cYkkXCIJl0jCJZJwiSRcIgmXSMIlknCJ5OasizqdTml3dnbWuBkbG3vp4/QVb1wiCZdIwiWScIkkXCIJl0jCJZJwiSRcIrk56wE/f/5s3MzPz5fOGhxs/pU+Pj6Wzupl3rhEEi6RhEsk4RJJuEQSLpGESyThEskFRA84ODho3CwsLJTOmp2dbdx8//69dFYv88YlknCJJFwiCZdIwiWScIkkXCIJl0jCJZKbs2d4+/Ztaffhw4fS7urqqnFT/Qd6S0tLjRs3Z/BKhEsk4RJJuEQSLpGESyThEkm4RGp1in/ZbrVa3X6WZ/n48WNpt7GxUdoNDw83biYnJ0tnjYyMlHZ/083NTeNmdHS0dFb10uNvqv5Mb1wiCZdIwiWScIkkXCIJl0jCJZJwiSRcIsXfnF1eXpZ279+/L+0ODw8bN6enp6Wz3r17V9qdnJw0bsbHx0tnVZ5/f3+/dNZrcHNGXxMukYRLJOESSbhEEi6RhEsk4RJJuESK/6d33759K+2+fPlS2v348aNxs7u7Wzrr6OiotOO/88YlknCJJFwiCZdIwiWScIkkXCIJl0jxn+5Un+vr16+l3fr6euOm3W6XzlpeXi7t9vb2Srv/A5/u0NeESyThEkm4RBIukYRLJOESSbhEEi6Ryjdn0Eu8cYkkXCIJl0jCJZJwiSRcIgmXSMIlknCJ9AcIdq8wHXZRYwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 5, Actual: 3. Wrong 6 times\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEZklEQVR4nO3dvy5saxiA8T3bjmREMjQSyTQKIiFKEeECZBIqChoFCQ0Jd6AVjUTEPeg0NCqJQqcQjfEnES0KhT9jX8L3nmPNWfOs8/zqN+9edp6s5su3pvT9/f39S4L5nfcDSP+G4QrJcIVkuEIyXCEZrpAMV0iGKyTDFdKf6GCpVGrmczRdZ2dnaK6npyc5Uy6XQ7sGBgZCc8PDw8mZarUa2jU5OZmcOT4+Du3a2NgIzWUpepDrG1dIhiskwxWS4QrJcIVkuEIyXCEZrpAMV0jhk7M8LC0tJWf29vZCuz4/P0NzHR0dobmIh4eH0Nzt7W1ypq+vL7SrUqkkZ56fn0O7WplvXCEZrpAMV0iGKyTDFZLhCslwhWS4QipFP3qXx9Wd9vb25Eyj0Qjtil4JifydXV1doV0LCwuhuc3NzeRMvV4P7VpcXEzO3N3dhXblwas7KjTDFZLhCslwhWS4QjJcIRmukAxXSIYrpJa+uvP+/p7ZrsHBwdDcyspKcmZ5eTm06+TkJDQ3OzubnLm4uAjtip4k0vnGFZLhCslwhWS4QjJcIRmukAxXSIYrpJa+upOlnZ2d0Nzc3Fxypq2tLbTr4+MjNPf6+pqcOTs7C+3a2tpKzjw9PYV25cGrOyo0wxWS4QrJcIVkuEIyXCEZrpAMV0iGK6T/zclZHqL/Z9VqNTkT/YBe5FpRrVYL7bq+vg7NZcmTMxWa4QrJcIVkuEIyXCEZrpAMV0iGKyTDFZInZwWzurqanJmeng7tmpqa+unj/GOenKnQDFdIhiskwxWS4QrJcIVkuEIyXCF5AFEwo6OjyZnoB/TK5XJo7uvrKzQX4QGECs1whWS4QjJcIRmukAxXSIYrJMMVkuEK6U/eD6Bs9fb2Jmfe3t5CuxqNxk8fp2l84wrJcIVkuEIyXCEZrpAMV0iGKyTDFZLhCsmTs4KJ/PTU/f19aFf0/lcefOMKyXCFZLhCMlwhGa6QDFdIhiskwxWSBxAFU6vVkjOnp6f/wZM0l29cIRmukAxXSIYrJMMVkuEKyXCFZLhCMlwh+XNREGNjY6G58/Pz5MzQ0FBo19XVVWguS/5clArNcIVkuEIyXCEZrpAMV0iGKyTDFVJLH0BE/s1W/r5Vlo6OjkJzkV/KmZmZ+enjNI0HECo0wxWS4QrJcIVkuEIyXCEZrpAMV0iGK6RcPnoXPYVbW1tLzhweHoZ2PT4+huayFP07d3d3kzMjIyOhXePj46E5Ot+4QjJcIRmukAxXSIYrJMMVkuEKyXCFZLhCauk7Z5VKJTmzv78f2nV5eRmaOzg4SM709/eHdm1vb4fmIvsmJiZCu+r1emiuVXnnTIVmuEIyXCEZrpAMV0iGKyTDFZLhCimXqztRLy8vyZn5+fnQrvX19dDczc1Ncqa7uzu0K/qhusjhAv1gIWu+cYVkuEIyXCEZrpAMV0iGKyTDFZLhCslwhRS+uiO1Et+4QjJcIRmukAxXSIYrJMMVkuEKyXCFZLhC+gvUcNSzdIF2ewAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 7, Actual: 8. Wrong 6 times\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAELklEQVR4nO3dsUsUbhzH8d9pBw4iNzSIBuliOOTUJBIE4tbQWoJziNBQ4J/Q0tQa4uwQTiE5t+kNQn9AEbg1ORiS1/iDW54vonifer3mDw83vHmWh7vrDAaDwX8QZuy2PwBchXCJJFwiCZdIwiWScIkkXCIJl0jCJdKd6rDT6dzk5xgZMzMzzc3jx49LZ/V6vdLuwYMHzc3q6mrprDdv3jQ3BwcHpbNuQ/Uh141LJOESSbhEEi6RhEsk4RJJuEQSLpGES6Tyy9moWlhYKO3ev39f2j158qS56Xa7pbNuw+LiYnMzyi9nVW5cIgmXSMIlknCJJFwiCZdIwiWScIkU/wDx/Pnz0m5tba20Oz8/b2729/dLZx0fH5d2p6enzc2nT59KZ/38+bO0S+fGJZJwiSRcIgmXSMIlknCJJFwiCZdIwiVSp/p3UaP6o3fj4+Ol3du3b0u7Z8+eNTcvX74snXV4eFjabW9vNzcXFxels969e1fajSo/esdfTbhEEi6RhEsk4RJJuEQSLpGES6T4B4jrtrGx0dxsbW2Vzvr8+XNp9/Tp0+Zmbm6udNbdu3ebm1+/fpXOug0eIPirCZdIwiWScIkkXCIJl0jCJZJwiSRcInk5u4JHjx6VdtWv7vR6vebm27dvpbPm5+ebm+rr1G3wcsZfTbhEEi6RhEsk4RJJuEQSLpGESyThEin+76Juw9HRUWn3+vXr0u7Dhw/NTfUVbpRfxa6TG5dIwiWScIkkXCIJl0jCJZJwiSRcInmAuEFTU1PXdlblx+z+JW5cIgmXSMIlknCJJFwiCZdIwiWScIkkXCL50bsruHOn9uDY7/dLu4cPHzY3Z2dnpbP8XRSMMOESSbhEEi6RhEsk4RJJuEQSLpGESyTfObuC9fX10q7yIlY1OTlZ2k1PTzc31b+eGmVuXCIJl0jCJZJwiSRcIgmXSMIlknCJ5AFiSLfbbW5evXp18x9kyPfv30u7Hz9+3PAnGQ1uXCIJl0jCJZJwiSRcIgmXSMIlknCJJFwieTkbcv/+/eZmaWmpdNbJyUlpNzbWvj8+fvxYOuv379+lXTo3LpGESyThEkm4RBIukYRLJOESSbhE8gAx5MWLF81N9R+Idnd3S7uJiYnmZmdnp3TWv8KNSyThEkm4RBIukYRLJOESSbhEEi6RhEukzmAwGJSGxdeidF++fGlulpeXS2dVf4Budna2uan+g8/Xr19Lu1FVzNGNSybhEkm4RBIukYRLJOESSbhEEi6RhEsk3zkbcnl5eW1n3bt3r7Tb29trbtJfxK6bG5dIwiWScIkkXCIJl0jCJZJwiSRcInmAGFL5cbmVlZXSWf1+v7Tb3Nws7fifG5dIwiWScIkkXCIJl0jCJZJwiSRcIgmXSOUfvYNR4sYlknCJJFwiCZdIwiWScIkkXCIJl0jCJdIfppStoavtC8QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_params(params):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(np.reshape(params, (16,16)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def calc_mistaken_predictions(confusion_stats):\n",
    "    mistakes_by_count = {}\n",
    "\n",
    "    for confusion_stat in confusion_stats:\n",
    "        test_X, test_Y, preds = confusion_stat\n",
    "        mistaken_indices = np.where(test_Y != preds)[0]\n",
    "\n",
    "        for idx in mistaken_indices:\n",
    "            key = (tuple(test_X[idx]), test_Y[idx], preds[idx])\n",
    "            mistakes_by_count[key] = mistakes_by_count.get(key, 0) + 1\n",
    "\n",
    "    return mistakes_by_count\n",
    "\n",
    "mistakes = calc_mistaken_predictions(q3_confusion_stats)\n",
    "worst_5 = sorted(mistakes.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "for dict_item in worst_5:\n",
    "    info_tuple = dict_item[0]\n",
    "    print(f\"Predicted: {info_tuple[2]}, Actual: {int(info_tuple[1])}. Wrong {dict_item[1]} times\")\n",
    "    visualize_params(info_tuple[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_kernel(c):\n",
    "    \"\"\"\n",
    "    Returns a Gaussian kernel with specified width\n",
    "\n",
    "    Args:\n",
    "        c: inverse width of gaussian kernel\n",
    "    Returns:\n",
    "        gaussian kernel function\n",
    "    \"\"\"\n",
    "    @jit(nopython=True)\n",
    "    def gaussian_kernel(p, q):\n",
    "        return np.exp(-c * np.sum((p - q) ** 2))\n",
    "    \n",
    "    return gaussian_kernel\n",
    "\n",
    "gaussian_params = np.linspace(0.004, 0.020, 5)\n",
    "gaussian_perceptrons = [MultiClassKernelPerceptron(get_gaussian_kernel(c)) for c in gaussian_params]\n",
    "gaussian_kernels = [p.get_kernel_matrix(p.kernel, data) for p in gaussian_perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "Currently running iter 1\n",
      "Currently running iter 2\n",
      "Currently running iter 3\n",
      "Currently running iter 4\n",
      "Currently running iter 5\n",
      "Currently running iter 6\n",
      "Currently running iter 7\n",
      "Currently running iter 8\n",
      "Currently running iter 9\n",
      "Currently running iter 10\n",
      "Currently running iter 11\n",
      "Currently running iter 12\n",
      "Currently running iter 13\n",
      "Currently running iter 14\n",
      "Currently running iter 15\n",
      "Currently running iter 16\n",
      "Currently running iter 17\n",
      "Currently running iter 18\n",
      "Currently running iter 19\n",
      "\n",
      "Test accuracy by kernel degree\n",
      "0.9836, 0.9891\n",
      "0.9861, 0.9911\n",
      "0.9860, 0.9908\n",
      "0.9871, 0.9906\n",
      "0.9859, 0.9912\n"
     ]
    }
   ],
   "source": [
    "q5_stats = run_20_iters(gaussian_perceptrons, gaussian_kernels, data)\n",
    "q5_summary = [get_confidence_range(runs) for runs in q5_stats]\n",
    "\n",
    "print()\n",
    "print(\"Test accuracy by gaussian kernel param\")\n",
    "for stat in q5_summary:\n",
    "    print(gaussian_params[i], \"-\", prettify_stat(stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "Currently running iter 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m q5_cv_stats, q5_cv_cs \u001b[39m=\u001b[39m run_cv_20_iters(gaussian_perceptrons, gaussian_kernels, data, gaussian_params)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Summary statistics!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m q5_cv_summary_accuracies \u001b[39m=\u001b[39m get_confidence_range(q5_cv_stats)\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m cv_indices \u001b[39m=\u001b[39m get_cross_validation_indices(training, num_folds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Pre-calculate kernel matrices\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m shuffled_kernels \u001b[39m=\u001b[39m [split_kernels(k[shuffle_indices, :][:, shuffle_indices], split_index) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kernels]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m cv_accuracies \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m perceptrons]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Iterate through CV sets\u001b[39;00m\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m cv_indices \u001b[39m=\u001b[39m get_cross_validation_indices(training, num_folds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Pre-calculate kernel matrices\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m shuffled_kernels \u001b[39m=\u001b[39m [split_kernels(k[shuffle_indices, :][:, shuffle_indices], split_index) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kernels]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m cv_accuracies \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m perceptrons]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y100sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Iterate through CV sets\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q5_cv_stats, q5_cv_cs = run_cv_20_iters(gaussian_perceptrons, gaussian_kernels, data, gaussian_params)\n",
    "\n",
    "# Summary statistics!\n",
    "q5_cv_summary_accuracies = get_confidence_range(q5_cv_stats)\n",
    "q5_cv_summary_cs = get_confidence_range(q5_cv_cs)\n",
    "\n",
    "print()\n",
    "print(\"Cross-validated accuracy\")\n",
    "print(prettify_stat(q5_cv_summary_accuracies))\n",
    "print(\"Cross-validated c's\")\n",
    "print(prettify_stat(q5_cv_summary_cs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quesiton 6: All v All Multiclass Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllVAllPerceptron(MultiClassKernelPerceptron):\n",
    "    \"\"\"\n",
    "    An All v All multiclass implementation of the kernel perceptron\n",
    "    Designed for the MNIST dataset to classify digits 0-9\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, kernel, num_digits=10):\n",
    "        \"\"\"\n",
    "        Instantiates the perceptron instance\n",
    "\n",
    "        Args:\n",
    "            kernel: kernel function to use\n",
    "            num_digits: optional param- number of digits to classify,\n",
    "                assumes digits range from 0...num_digits\n",
    "        \"\"\"\n",
    "        super().__init__(kernel, num_digits)\n",
    "\n",
    "    def train(self, X, Y, kernel_matrix=None, epochs=10):   \n",
    "        \"\"\"\n",
    "        Trains the perceptron\n",
    "\n",
    "        Args:\n",
    "            X: training data (n x d)\n",
    "            Y: training labels (n x 1)\n",
    "            kernel_matrix: optional pre-calculated kernel matrix\n",
    "            epochs: number of epochs to run\n",
    "        \"\"\"\n",
    "        self.X_training = X\n",
    "        Y = Y.astype(int)\n",
    "\n",
    "        # Calculate gram matrix if necessary\n",
    "        if kernel_matrix is None:\n",
    "            kernel_matrix = self.get_kernel_matrix(self.kernel, X)\n",
    "\n",
    "        # Find the indices of Y (and X) that correspond to each digit\n",
    "        self.indices_of_digits = []\n",
    "        for i in range(self.num_classes):\n",
    "            digit_indices = np.where(Y == i)[0]\n",
    "            self.indices_of_digits.append(digit_indices)\n",
    "            \n",
    "        self.counts_by_digit = [len(indices) for indices in self.indices_of_digits]\n",
    "\n",
    "        # There are only (10x9)/2=45 unique pairs, but let's use symmetry and referencing\n",
    "        #   to make life a little easier for us. (x,y) and (y,x) will point to the same array\n",
    "        self.alphas = np.empty((self.num_classes, self.num_classes), dtype=object)\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(i+1, self.num_classes):\n",
    "                total_data = self.counts_by_digit[i] + self.counts_by_digit[j]\n",
    "                \n",
    "                # Store the current index as the last element of the array\n",
    "                current_alpha = np.zeros(total_data+1, dtype=np.float32)\n",
    "                self.alphas[i, j] = current_alpha\n",
    "                self.alphas[j, i] = current_alpha\n",
    "\n",
    "        # Get the sub-kernel indices that correspond to each classifier\n",
    "        self.kernel_indices = np.empty((self.num_classes, self.num_classes), dtype=object)\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(i+1, self.num_classes):\n",
    "                current_indices = np.sort(np.append(self.indices_of_digits[i], self.indices_of_digits[j]))\n",
    "                self.kernel_indices[i, j] = current_indices\n",
    "                self.kernel_indices[j, i] = current_indices\n",
    "\n",
    "        num_data = kernel_matrix.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_data):\n",
    "                label = Y[i]\n",
    "                subkernel = kernel_matrix[i]\n",
    "                for digit, alpha in enumerate(self.alphas[label,:]):\n",
    "                    if (digit == label): continue\n",
    "                    \n",
    "                    # We only want the kernel corresponding to the current two digits!\n",
    "                    data_kernel = subkernel[self.kernel_indices[digit,label]]\n",
    "                    decision = np.sign(np.dot(alpha[:-1], data_kernel))\n",
    "\n",
    "                    # Assume that positive values = smaller label, negative values = bigger label\n",
    "                    correct_sign = -1 if digit < label else 1\n",
    "                    if (decision != correct_sign):\n",
    "                        # We are storing the current index in alpha[-1]\n",
    "                        alpha[int(alpha[-1])] += correct_sign\n",
    "                    \n",
    "                    # We are using alpha[-1] to store the index of the data. Increment it here\n",
    "                    alpha[-1] = (alpha[-1] + 1) % (len(alpha)-1)\n",
    "                \n",
    "    def predict(self, X, kernel_matrix=None):\n",
    "        \"\"\"\n",
    "        Predicts the labels of a new set of data\n",
    "\n",
    "        Args:\n",
    "            X: data to classify\n",
    "            kernel_matrix: an optional parameter for a precalculated kernel\n",
    "                between X and the training data\n",
    "        \"\"\"\n",
    "        y_hats = []\n",
    "        if kernel_matrix is None:\n",
    "            kernel_matrix = self.get_kernel_matrix(self.kernel, X, self.X_training)\n",
    "        \n",
    "        for data_index, data in enumerate(X):\n",
    "            predictions = np.zeros(self.num_classes)\n",
    "            for i in range(self.num_classes):\n",
    "                for j in range(i+1, self.num_classes):\n",
    "                    # Get the sub-kernel that corresponds to the (i, j) digits\n",
    "                    cur_indices = self.kernel_indices[i,j]\n",
    "                    cur_kernel = kernel_matrix[data_index,cur_indices]\n",
    "                    # The last alpha entry is our count! Discard for prediction\n",
    "                    cur_alpha = self.alphas[i,j][:-1]\n",
    "\n",
    "                    # Our classifiers are set up to always predict the smaller class\n",
    "                    #   Thus, a positive value indicates class i\n",
    "                    cur_pred = i if np.dot(cur_kernel, cur_alpha) >= 0 else j\n",
    "                    predictions[cur_pred] += 1\n",
    "            prediction = np.argmax(predictions)\n",
    "            y_hats.append(prediction)\n",
    "\n",
    "        return y_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "Currently running iter 1\n",
      "Currently running iter 2\n",
      "Currently running iter 3\n",
      "Currently running iter 4\n",
      "\n",
      "Test accuracy by kernel degree (1=7)\n",
      "1 - 0.9805, 0.9862\n",
      "2 - 0.9861, 0.9909\n",
      "3 - 0.9859, 0.9911\n",
      "4 - 0.9860, 0.9916\n",
      "5 - 0.9835, 0.9885\n",
      "6 - 0.9829, 0.9866\n",
      "7 - 0.9832, 0.9863\n"
     ]
    }
   ],
   "source": [
    "allvall_perceptrons = [AllVAllPerceptron(get_polynomial_kernel(p)) for p in poly_params]\n",
    "q6_stats = run_20_iters(allvall_perceptrons, poly_kernels, data)\n",
    "\n",
    "print()\n",
    "print(\"Test accuracy by kernel degree (1=7)\")\n",
    "for i, stat in enumerate(q6_stats):\n",
    "    print(poly_params[i], \"-\", prettify_stat(get_confidence_range(stat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "Currently running iter 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m q6_stats_b, q6_ds \u001b[39m=\u001b[39m run_cv_20_iters(allvall_perceptrons, poly_kernels, data, poly_params)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m q6_summary_accuracies \u001b[39m=\u001b[39m get_confidence_range(q6_stats_b)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m q6_summary_degrees \u001b[39m=\u001b[39m get_confidence_range(q6_ds)\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m full_training_kernel \u001b[39m=\u001b[39m shuffled_kernels[i][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m training_kernel, validation_kernel \u001b[39m=\u001b[39m get_training_validation_kernels(full_training_kernel, start, end)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m perceptron\u001b[39m.\u001b[39;49mtrain(train_X, train_Y, training_kernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m predictions \u001b[39m=\u001b[39m perceptron\u001b[39m.\u001b[39mpredict(valid_X, validation_kernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m accuracy \u001b[39m=\u001b[39m calculate_accuracy(predictions, valid_Y)\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 25\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     alpha[\u001b[39mint\u001b[39m(alpha[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m correct_sign\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# We are using alpha[-1] to store the index of the data. Increment it here\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#Y105sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m alpha[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m (alpha[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(alpha)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q6_stats_b, q6_ds = run_cv_20_iters(allvall_perceptrons, poly_kernels, data, poly_params)\n",
    "\n",
    "q6_summary_accuracies = get_confidence_range(q6_stats_b)\n",
    "q6_summary_degrees = get_confidence_range(q6_ds)\n",
    "\n",
    "print(\"Cross-validated accuracy\")\n",
    "print(prettify_stat(q6_summary_accuracies))\n",
    "print(\"Cross-validated degrees\")\n",
    "print(prettify_stat(q6_summary_degrees))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
