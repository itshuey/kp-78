{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Multiclass Kernel Perceptrons\n",
    "IDs: <> and <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassKernelPerceptron():\n",
    "    \"\"\"\n",
    "    A multiclass implementation of the kernel perceptron\n",
    "    Designed for the MNIST dataset to classify digits 0-9\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, kernel, num_digits=10):\n",
    "        \"\"\"\n",
    "        Instantiates the perceptron instance\n",
    "\n",
    "        Args:\n",
    "            kernel: kernel function to use\n",
    "            num_digits: optional param- number of digits to classify,\n",
    "                assumes digits range from 0...num_digits\n",
    "        \"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.num_classes = num_digits\n",
    "\n",
    "    def train(self, X, Y, kernel_matrix=None, epochs=10):   \n",
    "        \"\"\"\n",
    "        Trains the perceptron\n",
    "\n",
    "        Args:\n",
    "            X: training data (n x d)\n",
    "            Y: training labels (n x 1)\n",
    "            kernel_matrix: optional pre-calculated kernel matrix\n",
    "            epochs: number of epochs to run\n",
    "        \"\"\"\n",
    "        self.X_training = X\n",
    "        Y = Y.astype(int)\n",
    "\n",
    "        # Calculate gram matrix if necessary\n",
    "        if kernel_matrix is not None:\n",
    "            self.gram_matrix = kernel_matrix\n",
    "        else: \n",
    "            self.gram_matrix = self.get_kernel_matrix(self.kernel, X)\n",
    "\n",
    "        self.alpha = self.update_alpha(self.gram_matrix, Y, self.num_classes, epochs)\n",
    "    \n",
    "    # Pulling this into a separate function for numba,\n",
    "    #   but consider this as if it were appended to the train method\n",
    "    @staticmethod\n",
    "    # @jit(nopython=True)\n",
    "    def update_alpha(gram_matrix, Y, num_classes, epochs):\n",
    "        \n",
    "        num_data = gram_matrix.shape[0]\n",
    "        alpha = np.zeros((num_data, num_classes), dtype=np.float32)\n",
    "\n",
    "        # To boost efficiency, let's keep track of non-zero alpha weights\n",
    "        # Note that a data index is nontrivial if it is nonzero for ANY perceptron\n",
    "        nontrivial_indices = np.array([], dtype=np.int32)\n",
    "\n",
    "        # Instantiate once and reuse in update step\n",
    "        label_vector = -1 * np.ones(num_classes, dtype=np.float32)\n",
    "        zeros = np.zeros(num_classes, dtype=np.float32)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i in range(num_data):\n",
    "                # Get decisions for all the classifiers\n",
    "                if (nontrivial_indices.shape[0] == 0):\n",
    "                    decisions = zeros\n",
    "                else:\n",
    "                    # Pull out the relevant values of alpha and the gram matrix\n",
    "                    decisions = np.sign(np.dot(alpha[nontrivial_indices].T, gram_matrix[:, i][nontrivial_indices]))\n",
    "\n",
    "                # The label vector should be -1 for every index\n",
    "                # except the one corresponding to the current Yi label\n",
    "                label_vector[Y[i]] = 1\n",
    "\n",
    "                # For a given classifier, if the decision * label <= 0,\n",
    "                # the update is the label, otherwise it is zero\n",
    "                update = np.where(decisions*label_vector <=0, label_vector, zeros)\n",
    "\n",
    "                # Add the update to alpha, reset label vector\n",
    "                if (np.any(update != 0) and not np.any(alpha[i,:] != 0)):\n",
    "                    nontrivial_indices = np.append(nontrivial_indices, i)\n",
    "\n",
    "                alpha[i,:] += update\n",
    "                label_vector[Y[i]] = -1\n",
    "        \n",
    "        return alpha\n",
    "                \n",
    "    def predict(self, X, kernel_matrix=None):\n",
    "        \"\"\"\n",
    "        Predicts the labels of a new set of data\n",
    "\n",
    "        Args:\n",
    "            X: data to classify\n",
    "            kernel_matrix: an optional parameter for a precalculated kernel\n",
    "                between X and the training data\n",
    "        \"\"\"\n",
    "        # If we need to compute a kernel matrix, we only need to find distances\n",
    "        #   where the alphas are not 0.\n",
    "        relevant_indices = np.where(np.sum(self.alpha != 0, axis=1) > 0)[0]\n",
    "        if kernel_matrix is None:\n",
    "            kernel_matrix = self.get_kernel_matrix(self.kernel, X, self.X_training[relevant_indices])\n",
    "        else: \n",
    "            kernel_matrix = kernel_matrix[:, :][:, relevant_indices]\n",
    "        \n",
    "        alpha = self.alpha[relevant_indices]\n",
    "\n",
    "        # See how confident each classifier is that a given point is in that class\n",
    "        #   i.e. the distance between the point and the decision boundary\n",
    "        # We want to get a N x K matrix where N = number of classifiers, K = num data in X\n",
    "        # Each value is given by the dot product of the alphas for the classifier\n",
    "        #   and the distances from the data point to the training examples\n",
    "        data_confidence_by_classifier = kernel_matrix @ alpha\n",
    "        \n",
    "        # Pick the label of the classifier with the max confidence\n",
    "        # Each classifier operates on a 1-v-All basis\n",
    "        predictions = np.argmax(data_confidence_by_classifier, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def get_kernel_matrix(kernel_func, X1, X2=None):\n",
    "        \"\"\"\n",
    "        Calculates the full kernel matrix between X1 and X2\n",
    "        using an input kernel function\n",
    "\n",
    "        Args:\n",
    "            kernel_func: kernel function\n",
    "            X1: Ax1 vector\n",
    "            X2: optional Bx1 vector\n",
    "        Returns:\n",
    "            (AxB) matrix of kernel distances if X2 is given\n",
    "            (AxA) matrix otherwise\n",
    "        \"\"\"\n",
    "        A = X1.shape[0]\n",
    "        if X2 is None:\n",
    "            # In one-matrix case, we can use symmetry\n",
    "            kernel_matrix = np.empty((A,A), dtype=np.float32)\n",
    "            for i in range(A):\n",
    "                for j in range(i,A):\n",
    "                    kernel_matrix[i, j] = kernel_func(X1[i], X1[j])\n",
    "                    kernel_matrix[j, i] = kernel_matrix[i, j]\n",
    "        else:\n",
    "            B = X2.shape[0]\n",
    "            kernel_matrix = np.empty((A,B), dtype=np.float32)\n",
    "            for i in range(A):\n",
    "                for j in range(B):\n",
    "                    kernel_matrix[i, j] = kernel_func(X1[i], X2[j])\n",
    "        return kernel_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_fraction):\n",
    "    \"\"\"\n",
    "    Splits the data into two sets with the specified fraction.\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "        split_fraction: fraction to split\n",
    "    Returns:\n",
    "        larger dataset, smaller dataset, shuffled indices, and split index\n",
    "    \"\"\"\n",
    "    num_data = data.shape[0]\n",
    "    indices = np.arange(num_data)\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data = data[indices]\n",
    "\n",
    "    split_index = int(num_data * split_fraction)\n",
    "    return shuffled_data[:split_index], shuffled_data[split_index:], indices, split_index\n",
    "\n",
    "def get_cross_validation_indices(data, num_folds):\n",
    "    \"\"\"\n",
    "    Gets the indices of the data that represent a cross-validation split\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "        num_folds: number of folds\n",
    "    Returns:\n",
    "        list of (start, end) tuples\n",
    "    \"\"\"\n",
    "    num_data = data.shape[0]\n",
    "    data_per_fold = int(num_data / num_folds)\n",
    "\n",
    "    start = 0\n",
    "    index_tuples = []\n",
    "    for _ in range(num_folds-1):\n",
    "        index_tuples.append((start, start + data_per_fold))\n",
    "        start += data_per_fold\n",
    "    \n",
    "    index_tuples.append((start, num_data))\n",
    "    return index_tuples\n",
    "\n",
    "def get_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Splits the data from the labels\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "    Returns:\n",
    "        data, labels\n",
    "    \"\"\"\n",
    "    return data[:,1:], data[:,0]\n",
    "\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    \"\"\"\n",
    "    Returns predicted accuracy\n",
    "\n",
    "    Args:\n",
    "        predictions: numpy array of predictions\n",
    "        actual: actual values\n",
    "    Returns:\n",
    "        accuracy float\n",
    "    \"\"\"\n",
    "    num_correct = np.sum(predictions == actual)\n",
    "    return num_correct / actual.shape[0]\n",
    "\n",
    "def get_polynomial_kernel(degree):\n",
    "    \"\"\"\n",
    "    Returns a polynomial kernel with specified degree\n",
    "\n",
    "    Args:\n",
    "        degree: degree of polynomial\n",
    "    Returns:\n",
    "        polynomial kernel function\n",
    "    \"\"\"\n",
    "    @jit(nopython=True)\n",
    "    def polynomial_kernel(x1, x2):\n",
    "        return (np.dot(x1, x2)) ** degree\n",
    "    \n",
    "    return polynomial_kernel\n",
    "\n",
    "def split_kernels(kernel, split_index):\n",
    "    \"\"\"\n",
    "    Returns a training and test kernel from an index to split\n",
    "\n",
    "    Args:\n",
    "        kernel: NxN gram matrix\n",
    "        split_index: index < N, number of training examples\n",
    "    Returns:\n",
    "        training kernel matrix (square), test kernel matrix (rectangle)\n",
    "    \"\"\"\n",
    "    # Grab the first split_index elements for the square training kernel\n",
    "    #   and one of the remaining rectangles for the test kernel \n",
    "    training = kernel[:split_index,:split_index]\n",
    "    test = kernel[split_index:,:split_index]\n",
    "    return training, test\n",
    "\n",
    "def get_training_validation_kernels(kernel, start, end):\n",
    "    \"\"\"\n",
    "    Returns a training and validation kernel from cross-validation indices\n",
    "    The start-end defines the validation region\n",
    "\n",
    "    Args:\n",
    "        kernel: NxN gram matrix\n",
    "        start, end: indices < N of the validation region\n",
    "    Returns:\n",
    "        training kernel matrix (square), validation kernel matrix (rectangle)\n",
    "    \"\"\"\n",
    "    # Build training array by picturing four corners of a square\n",
    "    left = np.vstack((kernel[:start,:start], kernel[end:,:start]))\n",
    "    right = np.vstack((kernel[:start,end:], kernel[end:,end:]))\n",
    "    training = np.hstack((left, right))\n",
    "\n",
    "    # Build validation array by picturing a rectangle\n",
    "    valid = np.hstack((kernel[start:end, :start], kernel[start:end, end:]))\n",
    "    return training, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Basic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "data = np.genfromtxt('./zipcombo.dat')\n",
    "\n",
    "# Define polynomial perceptrons, pre-calculate kernels\n",
    "poly_perceptrons = [MultiClassKernelPerceptron(get_polynomial_kernel(d+1)) for d in range(7)]\n",
    "poly_kernels = [p.get_kernel_matrix(p.kernel, data) for p in poly_perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 1\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 2\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 3\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 4\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 5\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 6\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 7\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 8\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 9\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 10\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 11\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 12\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 13\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 14\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 15\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 16\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 17\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 18\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 19\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n"
     ]
    }
   ],
   "source": [
    "def run_20_iters(perceptrons, kernels):\n",
    "    \"\"\"\n",
    "    Follows the routine in Q1 of the coursework: \n",
    "        Reports the best accuracy of the perceptrons over 20 iterations\n",
    "\n",
    "    Args:\n",
    "        perceptrons: perceptrons to use\n",
    "        kernels: pre-calculated kernels corresponding to the perceptrons\n",
    "    Returns:\n",
    "        accuracy stats for the 20 runs for each perceptron\n",
    "    \"\"\"\n",
    "        \n",
    "    stats = [[] for _ in range(len(perceptrons))]\n",
    "    for iter in range(20):\n",
    "        print(\"Currently running iter\", iter)\n",
    "        training, test, shuffle_indices, split_index = split_data(data, 0.8)\n",
    "        train_X, train_Y = get_data_and_labels(training)\n",
    "        test_X, test_Y = get_data_and_labels(test)\n",
    "\n",
    "        for i, perceptron in enumerate(perceptrons):\n",
    "            print(\"> Perceptron\", i)\n",
    "            shuffled_kernel = kernels[i][shuffle_indices, :][:, shuffle_indices]\n",
    "            training_kernel, test_kernel = split_kernels(shuffled_kernel, split_index)\n",
    "\n",
    "            perceptron.train(train_X, train_Y, training_kernel)\n",
    "            predictions = perceptron.predict(test_X, test_kernel)\n",
    "            accuracy = calculate_accuracy(predictions, test_Y)\n",
    "            stats[i].append(accuracy)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "q1_stats = run_20_iters(poly_perceptrons, poly_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy by kernel degree\n",
      "0.9459 0.9579\n",
      "0.9816 0.9885\n",
      "0.9839 0.9895\n",
      "0.9838 0.9893\n",
      "0.9822 0.9882\n",
      "0.9821 0.9875\n",
      "0.9806 0.9865\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics!\n",
    "def get_confidence_range(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return [mean-std, mean+std]\n",
    "\n",
    "q1_summary = [get_confidence_range(runs) for runs in q1_stats]\n",
    "\n",
    "print(\"Test accuracy by kernel degree\")\n",
    "for stat in q1_summary:\n",
    "    print(\"{:.4f}\".format(stat[0]),\"{:.4f}\".format(stat[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "> Cross-validation (0, 1487)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      ">> Perceptron 3\n",
      ">> Perceptron 4\n",
      ">> Perceptron 5\n",
      ">> Perceptron 6\n",
      "> Cross-validation (1487, 2974)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      ">> Perceptron 3\n",
      ">> Perceptron 4\n",
      ">> Perceptron 5\n",
      ">> Perceptron 6\n",
      "> Cross-validation (2974, 4461)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      ">> Perceptron 3\n",
      ">> Perceptron 4\n",
      ">> Perceptron 5\n",
      ">> Perceptron 6\n",
      "> Cross-validation (4461, 5948)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      ">> Perceptron 3\n",
      ">> Perceptron 4\n",
      ">> Perceptron 5\n",
      ">> Perceptron 6\n",
      "> Cross-validation (5948, 7438)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      ">> Perceptron 3\n",
      ">> Perceptron 4\n",
      ">> Perceptron 5\n",
      ">> Perceptron 6\n"
     ]
    }
   ],
   "source": [
    "def run_cv_20_iters(perceptrons, kernels, confusion_stats=None):\n",
    "    \"\"\"\n",
    "    Follows the routine in Q2 of the coursework: \n",
    "        Performs 5-fold CV to find the optimal hyperparameter\n",
    "        Optionally outputs confusion stats\n",
    "\n",
    "    Args:\n",
    "        perceptrons: perceptrons to use\n",
    "        kernels: pre-calculated kernels corresponding to the perceptrons\n",
    "        confusion_stats: optional array to store confusion stats\n",
    "    Returns:\n",
    "        average accuracy, degree for the different runs\n",
    "    \"\"\"\n",
    "    accuracy_stats = []\n",
    "    best_degrees = []\n",
    "\n",
    "    for iter in range(1):\n",
    "        print(\"Currently running iter\", iter)\n",
    "        training, test, shuffle_indices, split_index = split_data(data, 0.8)\n",
    "        test_X, test_Y = get_data_and_labels(test)\n",
    "        full_train_X, full_train_Y = get_data_and_labels(training)\n",
    "\n",
    "        # Cross-Validation step\n",
    "        num_folds = 5\n",
    "        cv_indices = get_cross_validation_indices(training, num_folds)\n",
    "\n",
    "        # Pre-calculate kernel matrices\n",
    "        shuffled_kernels = [split_kernels(k[shuffle_indices, :][:, shuffle_indices], split_index) for k in kernels]\n",
    "        cv_accuracies = [[] for _ in perceptrons]\n",
    "        \n",
    "        # Iterate through CV sets\n",
    "        for cur_idx in cv_indices:\n",
    "            print(\"> Cross-validation\", cur_idx)\n",
    "            start, end = cur_idx\n",
    "            cur_valid = training[start:end,:]\n",
    "            cur_train = np.concatenate((training[:start,:], training[end:,:]))\n",
    "\n",
    "            train_X, train_Y = get_data_and_labels(cur_train)\n",
    "            valid_X, valid_Y = get_data_and_labels(cur_valid)\n",
    "\n",
    "            # For each set, evaluate each perceptron\n",
    "            for i, perceptron in enumerate(perceptrons):\n",
    "                print(\">> Perceptron\", i)\n",
    "                # Grab the training kernel of the i'th shuffled kernel\n",
    "                full_training_kernel = shuffled_kernels[i][0]\n",
    "                training_kernel, validation_kernel = get_training_validation_kernels(full_training_kernel, start, end)\n",
    "                perceptron.train(train_X, train_Y, training_kernel)\n",
    "                predictions = perceptron.predict(valid_X, validation_kernel)\n",
    "\n",
    "                accuracy = calculate_accuracy(predictions, valid_Y)\n",
    "                cv_accuracies[i].append(accuracy)\n",
    "        \n",
    "        # Choose the best perceptron, use it to train the whole set\n",
    "        best_perceptron = np.argmax(np.mean(cv_accuracies, axis=1))\n",
    "        best_degrees.append(best_perceptron + 1)\n",
    "\n",
    "        # Leverage pre-computed kernel matrix\n",
    "        training_kernel, test_kernel = shuffled_kernels[best_perceptron]\n",
    "\n",
    "        chosen_perceptron = perceptrons[best_perceptron]\n",
    "        chosen_perceptron.train(full_train_X,full_train_Y,training_kernel)\n",
    "        test_predictions = chosen_perceptron.predict(test_X, test_kernel)\n",
    "        test_accuracy = calculate_accuracy(test_predictions, test_Y)\n",
    "        accuracy_stats.append(test_accuracy)\n",
    "\n",
    "        if confusion_stats is not None:\n",
    "            confusion_stats.append((test_X, test_Y, test_predictions))\n",
    "\n",
    "        return accuracy_stats, best_degrees\n",
    "\n",
    "q3_confusion_stats = []\n",
    "q2_stats, q2_ds = run_cv_20_iters(poly_perceptrons, poly_kernels, q3_confusion_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy\n",
      "[0.9897849462365591, 0.9897849462365591]\n",
      "Cross-validated degrees\n",
      "[5.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics!\n",
    "q2_summary_accuracies = get_confidence_range(q2_stats)\n",
    "q2_summary_degrees = get_confidence_range(q2_ds)\n",
    "\n",
    "print(\"Cross-validated accuracy\")\n",
    "print(q2_summary_accuracies)\n",
    "print(\"Cross-validated degrees\")\n",
    "print(q2_summary_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Confusion Matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False  True False False  True False  True  True]\n"
     ]
    }
   ],
   "source": [
    "def calc_confusion_stats(confusion_stat):\n",
    "    test_X, test_Y, preds = confusion_stat\n",
    "\n",
    "    digits, counts = np.unique(test_Y, return_counts=True)\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "\n",
    "    for i, y in enumerate(test_Y.astype(int)):\n",
    "        if (y != preds[i]):\n",
    "            confusion_matrix[y, preds[i]] += 1\n",
    "    \n",
    "    confusion_matrix2 = confusion_matrix / counts.T\n",
    "    return confusion_matrix2, confusion_matrix, counts\n",
    "\n",
    "test_q3_stats = calc_confusion_stats(q3_confusion_stats[0])\n",
    "print(test_q3_stats[0][3,:] == test_q3_stats[1][3,:]/test_q3_stats[2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Mistaken digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_params(params):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.reshape(params, (16,16)),\n",
    "                interpolation=\"None\",\n",
    "                cmap='gray',\n",
    "                vmin=0, \n",
    "                vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_params(data[0,1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0.0\n",
      "0.144\n",
      "> 0.0020689655172413794\n",
      "0.93\n",
      "> 0.004137931034482759\n",
      "0.946\n",
      "> 0.0062068965517241385\n",
      "0.948\n",
      "> 0.008275862068965517\n",
      "0.952\n",
      "> 0.010344827586206896\n",
      "0.954\n",
      "> 0.012413793103448277\n",
      "0.946\n",
      "> 0.014482758620689656\n",
      "0.958\n",
      "> 0.016551724137931035\n",
      "0.948\n",
      "> 0.018620689655172416\n",
      "0.946\n",
      "> 0.020689655172413793\n",
      "0.964\n",
      "> 0.022758620689655173\n",
      "0.956\n",
      "> 0.024827586206896554\n",
      "0.95\n",
      "> 0.02689655172413793\n",
      "0.948\n",
      "> 0.028965517241379312\n",
      "0.952\n",
      "> 0.03103448275862069\n",
      "0.946\n",
      "> 0.03310344827586207\n",
      "0.946\n",
      "> 0.03517241379310345\n",
      "0.946\n",
      "> 0.03724137931034483\n",
      "0.944\n",
      "> 0.039310344827586205\n",
      "0.946\n",
      "> 0.041379310344827586\n",
      "0.938\n",
      "> 0.043448275862068966\n",
      "0.928\n",
      "> 0.04551724137931035\n",
      "0.94\n",
      "> 0.04758620689655173\n",
      "0.93\n",
      "> 0.04965517241379311\n",
      "0.938\n",
      "> 0.05172413793103448\n",
      "0.936\n",
      "> 0.05379310344827586\n",
      "0.942\n",
      "> 0.05586206896551724\n",
      "0.93\n",
      "> 0.057931034482758624\n",
      "0.94\n",
      "> 0.06\n",
      "0.932\n"
     ]
    }
   ],
   "source": [
    "def get_gaussian_kernel(c):\n",
    "    \"\"\"\n",
    "    Returns a Gaussian kernel with specified width\n",
    "\n",
    "    Args:\n",
    "        c: inverse width of gaussian kernel\n",
    "    Returns:\n",
    "        gaussian kernel function\n",
    "    \"\"\"\n",
    "    @jit(nopython=True)\n",
    "    def gaussian_kernel(p, q):\n",
    "        return np.exp(-c * np.sum((p - q) ** 2))\n",
    "    return gaussian_kernel\n",
    "\n",
    "for c in np.linspace(0, .06, 30):\n",
    "    p_tst = MultiClassKernelPerceptron(get_gaussian_kernel(c))\n",
    "    xtr, ytr = get_data_and_labels(data[:2000,:])\n",
    "    p_tst.train(xtr, ytr)\n",
    "    xtst, ytst = get_data_and_labels(data[2000:2500,:])\n",
    "    y_hat = p_tst.predict(xtst)\n",
    "    print(\">\", c)\n",
    "    print(np.sum(y_hat == ytst)/y_hat.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
