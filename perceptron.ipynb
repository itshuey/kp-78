{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Multiclass Kernel Perceptrons\n",
    "IDs: <> and <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassKernelPerceptron():\n",
    "    \"\"\"\n",
    "    A multiclass implementation of the kernel perceptron\n",
    "    Designed for the MNIST dataset to classify digits 0-9\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, kernel, num_digits=10):\n",
    "        \"\"\"\n",
    "        Instantiates the perceptron instance\n",
    "\n",
    "        Args:\n",
    "            kernel: kernel function to use\n",
    "            num_digits: optional param- number of digits to classify,\n",
    "                assumes digits range from 0...num_digits\n",
    "        \"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.num_classes = num_digits\n",
    "\n",
    "    def train(self, X, Y, kernel_matrix=None):   \n",
    "        \"\"\"\n",
    "        Trains the perceptron\n",
    "\n",
    "        Args:\n",
    "            X: training data (n x d)\n",
    "            Y: training labels (n x 1)\n",
    "            kernel_matrix: optional pre-calculated kernel matrix\n",
    "        \"\"\"\n",
    "        max_epochs = 50\n",
    "        self.X_training = X\n",
    "        Y = Y.astype(int)\n",
    "\n",
    "        # Calculate gram matrix if necessary\n",
    "        if kernel_matrix is not None:\n",
    "            self.gram_matrix = kernel_matrix\n",
    "        else: \n",
    "            self.gram_matrix = self.get_kernel_matrix(self.kernel, X)\n",
    "\n",
    "        self.alpha = self.update_alpha(self.gram_matrix, Y, self.num_classes, max_epochs)\n",
    "    \n",
    "    # Pulling this into a separate function for numba,\n",
    "    #   but consider this as if it were appended to the train method\n",
    "    @jit(nopython=True)\n",
    "    def update_alpha(gram_matrix, Y, num_classes, max_epochs):\n",
    "        \n",
    "        num_data = gram_matrix.shape[0]\n",
    "        alpha = np.zeros((num_data, num_classes))\n",
    "\n",
    "        # Instantiate once and reuse in update step\n",
    "        label_vector = -1 * np.ones(num_classes)\n",
    "        zeros = np.zeros(num_classes)\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            for i in range(num_data):\n",
    "                # Get decisions for all the classifiers\n",
    "                decisions = np.sign(np.dot(alpha.T, gram_matrix[:, i]))\n",
    "\n",
    "                # The label vector should be -1 for every index\n",
    "                # except the one corresponding to the current Yi label\n",
    "                label_vector[Y[i]] = 1\n",
    "\n",
    "                # For a given classifier, if the decision * label <= 0,\n",
    "                # the update is the label, otherwise it is zero\n",
    "                update = np.where(decisions*label_vector <=0, label_vector, zeros)\n",
    "\n",
    "                # Add the update to alpha, reset label vector\n",
    "                alpha[i,:] += update\n",
    "                label_vector[Y[i]] = -1\n",
    "        \n",
    "        return alpha\n",
    "                \n",
    "    def predict(self, X, kernel_matrix=None):\n",
    "        \"\"\"\n",
    "        Predicts the labels of a new set of data\n",
    "\n",
    "        Args:\n",
    "            X: data to classify\n",
    "            kernel_matrix: an optional parameter for a precalculated kernel\n",
    "                between X and the training data\n",
    "        \"\"\"\n",
    "        # If we need to compute a kernel matrix, we only need to find distances\n",
    "        #   where the alphas are not 0.\n",
    "        if kernel_matrix is None:\n",
    "            relevant_indices = np.where(np.sum(self.alpha != 0, axis=1) > 0)[0]\n",
    "            kernel_matrix = self.get_kernel_matrix(self.kernel, X, self.X_training[relevant_indices])\n",
    "            alpha = self.alpha[relevant_indices]\n",
    "        else: \n",
    "            # In this case, we don't know what the indices are anymore, \n",
    "            #   so we can't use the previous trick\n",
    "            alpha = self.alpha\n",
    "\n",
    "        # See how confident each classifier is that a given point is in that class\n",
    "        #   i.e. the distance between the point and the decision boundary\n",
    "        # We want to get a N x K matrix where N = number of classifiers, K = num data in X\n",
    "        # Each value is given by the dot product of the alphas for the classifier\n",
    "        #   and the distances from the data point to the training examples\n",
    "        data_confidence_by_classifier = kernel_matrix @ alpha\n",
    "        \n",
    "        # Pick the label of the classifier with the max confidence\n",
    "        # Each classifier operates on a 1-v-All basis\n",
    "        predictions = np.argmax(data_confidence_by_classifier, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def get_kernel_matrix(kernel_func, X1, X2=None):\n",
    "        \"\"\"\n",
    "        Calculates the full kernel matrix between X1 and X2\n",
    "        using an input kernel function\n",
    "\n",
    "        Args:\n",
    "            kernel_func: kernel function\n",
    "            X1: Ax1 vector\n",
    "            X2: optional Bx1 vector\n",
    "        Returns:\n",
    "            (AxB) matrix of kernel distances if X2 is given\n",
    "            (AxA) matrix otherwise\n",
    "        \"\"\"\n",
    "        A = X1.shape[0]\n",
    "        if X2 is None:\n",
    "            # In one-matrix case, we can use symmetry\n",
    "            kernel_matrix = np.empty((A,A), dtype=np.float32)\n",
    "            for i in range(A):\n",
    "                for j in range(i,A):\n",
    "                    kernel_matrix[i, j] = kernel_func(X1[i], X1[j])\n",
    "                    kernel_matrix[j, i] = kernel_matrix[i, j]\n",
    "        else:\n",
    "            B = X2.shape[0]\n",
    "            kernel_matrix = np.empty((A,B), dtype=np.float32)\n",
    "            for i in range(A):\n",
    "                for j in range(B):\n",
    "                    kernel_matrix[i, j] = kernel_func(X1[i], X2[j])\n",
    "        return kernel_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_fraction):\n",
    "    \"\"\"\n",
    "    Splits the data into two sets with the specified fraction.\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "        split_fraction: fraction to split\n",
    "    Returns:\n",
    "        larger dataset, smaller dataset, shuffled indices, and split index\n",
    "    \"\"\"\n",
    "    num_data = data.shape[0]\n",
    "    indices = np.arange(num_data)\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data = data[indices]\n",
    "\n",
    "    split_index = int(num_data * split_fraction)\n",
    "    return shuffled_data[:split_index], shuffled_data[split_index:], indices, split_index\n",
    "\n",
    "def get_cross_validation_indices(data, num_folds):\n",
    "    \"\"\"\n",
    "    Gets the indices of the data that represent a cross-validation split\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "        num_folds: number of folds\n",
    "    Returns:\n",
    "        list of (start, end) tuples\n",
    "    \"\"\"\n",
    "    num_data = data.shape[0]\n",
    "    data_per_fold = int(num_data / num_folds)\n",
    "\n",
    "    start = 0\n",
    "    index_tuples = []\n",
    "    for _ in range(num_folds):\n",
    "        index_tuples.append((start, start + data_per_fold))\n",
    "        start += data_per_fold\n",
    "    \n",
    "    index_tuples.append((start, num_data))\n",
    "    return index_tuples\n",
    "\n",
    "def get_data_and_labels(data):\n",
    "    \"\"\"\n",
    "    Splits the data from the labels\n",
    "\n",
    "    Args:\n",
    "        data: numpy matrix of data\n",
    "    Returns:\n",
    "        data, labels\n",
    "    \"\"\"\n",
    "    return data[:,1:], data[:,0]\n",
    "\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    \"\"\"\n",
    "    Returns predicted accuracy\n",
    "\n",
    "    Args:\n",
    "        predictions: numpy array of predictions\n",
    "        actual: actual values\n",
    "    Returns:\n",
    "        accuracy float\n",
    "    \"\"\"\n",
    "    num_correct = np.sum(predictions == actual)\n",
    "    return num_correct / actual.shape[0]\n",
    "\n",
    "def get_polynomial_kernel(degree):\n",
    "    \"\"\"\n",
    "    Returns a polynomial kernel with specified degree\n",
    "\n",
    "    Args:\n",
    "        degree: degree of polynomial\n",
    "    Returns:\n",
    "        polynomial kernel function\n",
    "    \"\"\"\n",
    "    @jit(nopython=True)\n",
    "    def polynomial_kernel(x1, x2):\n",
    "        return (np.dot(x1, x2)) ** degree\n",
    "    \n",
    "    return polynomial_kernel\n",
    "\n",
    "def split_kernels(kernel, split_index):\n",
    "    \"\"\"\n",
    "    Returns a training and test kernel from an index to split\n",
    "\n",
    "    Args:\n",
    "        kernel: NxN gram matrix\n",
    "        split_index: index < N, number of training examples\n",
    "    Returns:\n",
    "        training kernel matrix (square), test kernel matrix (rectangle)\n",
    "    \"\"\"\n",
    "    # Grab the first split_index elements for the square training kernel\n",
    "    #   and one of the remaining rectangles for the test kernel \n",
    "    training = kernel[:split_index,:split_index]\n",
    "    test = kernel[split_index:,:split_index]\n",
    "    return training, test\n",
    "\n",
    "def get_training_validation_kernels(kernel, start, end):\n",
    "    \"\"\"\n",
    "    Returns a training and validation kernel from cross-validation indices\n",
    "    The start-end defines the validation region\n",
    "\n",
    "    Args:\n",
    "        kernel: NxN gram matrix\n",
    "        start, end: indices < N of the validation region\n",
    "    Returns:\n",
    "        training kernel matrix (square), validation kernel matrix (rectangle)\n",
    "    \"\"\"\n",
    "    # Build training array by picturing four corners of a square\n",
    "    left = np.vstack((kernel[:start,:start], kernel[end:,:start]))\n",
    "    right = np.vstack((kernel[:start,end:], kernel[end:,end:]))\n",
    "    training = np.hstack((left, right))\n",
    "\n",
    "    # Build validation array by picturing a rectangle\n",
    "    valid = np.hstack((kernel[start:end, :start], kernel[start:end, end:]))\n",
    "    return training, valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Basic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "data = np.genfromtxt('./zipcombo.dat')\n",
    "\n",
    "# Define polynomial perceptrons, pre-calculate kernels\n",
    "poly_perceptrons = [MultiClassKernelPerceptron(get_polynomial_kernel(d+1)) for d in range(7)]\n",
    "poly_kernels = [p.get_kernel_matrix(p.kernel, data) for p in poly_perceptrons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 1\n",
      "> Perceptron 0\n",
      "> Perceptron 1\n",
      "> Perceptron 2\n",
      "> Perceptron 3\n",
      "> Perceptron 4\n",
      "> Perceptron 5\n",
      "> Perceptron 6\n",
      "Currently running iter 2\n",
      "> Perceptron 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             stats[i]\u001b[39m.\u001b[39mappend(accuracy)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m stats\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m q1_stats \u001b[39m=\u001b[39m run_20_iters(poly_perceptrons, poly_kernels)\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m shuffled_kernel \u001b[39m=\u001b[39m kernels[i][shuffle_indices, :][:, shuffle_indices]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m training_kernel, test_kernel \u001b[39m=\u001b[39m split_kernels(shuffled_kernel, split_index)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m perceptron\u001b[39m.\u001b[39;49mtrain(train_X, train_Y, training_kernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m predictions \u001b[39m=\u001b[39m perceptron\u001b[39m.\u001b[39mpredict(test_X, test_kernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m accuracy \u001b[39m=\u001b[39m calculate_accuracy(predictions, test_Y)\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_data):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39m# Get decisions for all the classifiers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         decisions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgram_matrix[:, i]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39m# The label vector should be -1 for every index\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39m# except the one corresponding to the current Yi label\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#W2sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         label_vector[Y[i]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_20_iters(perceptrons, kernels):\n",
    "    \"\"\"\n",
    "    Follows the instructions in Q1 of the coursework: \n",
    "        Reports the best accuracy of the perceptrons over 20 iterations\n",
    "\n",
    "    Args:\n",
    "        perceptrons: perceptrons to use\n",
    "        kernels: pre-calculated kernels corresponding to the perceptrons\n",
    "    Returns:\n",
    "        accuracy stats for the 20 runs for each perceptron\n",
    "    \"\"\"\n",
    "        \n",
    "    stats = [[] for _ in range(len(perceptrons))]\n",
    "    for iter in range(20):\n",
    "        print(\"Currently running iter\", iter)\n",
    "        training, test, shuffle_indices, split_index = split_data(data, 0.8)\n",
    "        train_X, train_Y = get_data_and_labels(training)\n",
    "        test_X, test_Y = get_data_and_labels(test)\n",
    "\n",
    "        for i, perceptron in enumerate(perceptrons):\n",
    "            print(\"> Perceptron\", i)\n",
    "            shuffled_kernel = kernels[i][shuffle_indices, :][:, shuffle_indices]\n",
    "            training_kernel, test_kernel = split_kernels(shuffled_kernel, split_index)\n",
    "\n",
    "            perceptron.train(train_X, train_Y, training_kernel)\n",
    "            predictions = perceptron.predict(test_X, test_kernel)\n",
    "            accuracy = calculate_accuracy(predictions, test_Y)\n",
    "            stats[i].append(accuracy)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "q1_stats = run_20_iters(poly_perceptrons, poly_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy by kernel degree\n",
      "0.9022 0.9258\n",
      "0.9667 0.9774\n",
      "0.9672 0.9812\n",
      "0.9683 0.9796\n",
      "0.9677 0.9790\n",
      "0.9704 0.9704\n",
      "0.9629 0.9629\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics!\n",
    "q1_summary = [[np.mean(runs)-np.std(runs),\n",
    "               np.mean(runs)+np.std(runs)] \n",
    "    for runs in q1_stats]\n",
    "\n",
    "print(\"Test accuracy by kernel degree\")\n",
    "for stat in q1_summary:\n",
    "    print(\"{:.4f}\".format(stat[0]),\"{:.4f}\".format(stat[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running iter 0\n",
      "> Cross-validation (0, 1487)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      "> Cross-validation (1487, 2974)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      "> Cross-validation (2974, 4461)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      "> Cross-validation (4461, 5948)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      "> Cross-validation (5948, 7435)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      "> Cross-validation (7435, 7438)\n",
      ">> Perceptron 0\n",
      ">> Perceptron 1\n",
      ">> Perceptron 2\n",
      "Currently running iter 1\n",
      "> Cross-validation (0, 1487)\n",
      ">> Perceptron 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         test_accuracy \u001b[39m=\u001b[39m calculate_accuracy(test_predictions, test_Y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         accuracy_stats\u001b[39m.\u001b[39mappend(test_accuracy)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m q2_stats, q2_ds \u001b[39m=\u001b[39m run_cv_20_iters(poly_perceptrons[:\u001b[39m3\u001b[39;49m], poly_kernels[:\u001b[39m3\u001b[39;49m])\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m full_training_kernel \u001b[39m=\u001b[39m shuffled_kernels[i][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m training_kernel, validation_kernel \u001b[39m=\u001b[39m get_training_validation_kernels(full_training_kernel, start, end)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m perceptron\u001b[39m.\u001b[39;49mtrain(train_X, train_Y, training_kernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m predictions \u001b[39m=\u001b[39m perceptron\u001b[39m.\u001b[39mpredict(valid_X, validation_kernel)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m accuracy \u001b[39m=\u001b[39m calculate_accuracy(predictions, valid_Y)\n",
      "\u001b[1;32m/Users/huey/Documents/Supervised Learning/78-cw2/perceptron.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_data):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39m# Get decisions for all the classifiers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         decisions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgram_matrix[:, i]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39m# The label vector should be -1 for every index\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39m# except the one corresponding to the current Yi label\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huey/Documents/Supervised%20Learning/78-cw2/perceptron.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         label_vector[Y[i]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_cv_20_iters(perceptrons, kernels):\n",
    "    accuracy_stats = []\n",
    "    best_degrees = []\n",
    "\n",
    "    for iter in range(20):\n",
    "        print(\"Currently running iter\", iter)\n",
    "        training, test, shuffle_indices, split_index = split_data(data, 0.8)\n",
    "        test_X, test_Y = get_data_and_labels(test)\n",
    "        full_train_X, full_train_Y = get_data_and_labels(training)\n",
    "\n",
    "        # Cross-Validation step\n",
    "        num_folds = 5\n",
    "        cv_indices = get_cross_validation_indices(training, num_folds)\n",
    "\n",
    "        # Pre-calculate kernel matrices\n",
    "        shuffled_kernels = [split_kernels(k[shuffle_indices, :][:, shuffle_indices], split_index) for k in kernels]\n",
    "        cv_accuracies = [[] for _ in perceptrons]\n",
    "        \n",
    "        # Iterate through CV sets\n",
    "        for cur_idx in cv_indices:\n",
    "            print(\"> Cross-validation\", cur_idx)\n",
    "            start, end = cur_idx\n",
    "            cur_valid = training[start:end,:]\n",
    "            cur_train = np.concatenate((training[:start,:], training[end:,:]))\n",
    "\n",
    "            train_X, train_Y = get_data_and_labels(cur_train)\n",
    "            valid_X, valid_Y = get_data_and_labels(cur_valid)\n",
    "\n",
    "            # For each set, evaluate each perceptron\n",
    "            for i, perceptron in enumerate(perceptrons):\n",
    "                print(\">> Perceptron\", i)\n",
    "                # Grab the training kernel of the i'th shuffled kernel\n",
    "                full_training_kernel = shuffled_kernels[i][0]\n",
    "                training_kernel, validation_kernel = get_training_validation_kernels(full_training_kernel, start, end)\n",
    "                perceptron.train(train_X, train_Y, training_kernel)\n",
    "                predictions = perceptron.predict(valid_X, validation_kernel)\n",
    "\n",
    "                accuracy = calculate_accuracy(predictions, valid_Y)\n",
    "                cv_accuracies[i].append(accuracy)\n",
    "        \n",
    "        # Choose the best perceptron, use it to train the whole set\n",
    "        best_perceptron = np.argmax(np.mean(cv_accuracies, axis=1))\n",
    "        best_degrees.append(best_perceptron + 1)\n",
    "\n",
    "        # Leverage pre-computed kernel matrix\n",
    "        training_kernel, test_kernel = shuffled_kernels[best_perceptron]\n",
    "\n",
    "        chosen_perceptron = perceptrons[best_perceptron]\n",
    "        chosen_perceptron.train(full_train_X,full_train_Y,training_kernel)\n",
    "        test_predictions = chosen_perceptron.predict(test_X, test_kernel)\n",
    "        test_accuracy = calculate_accuracy(test_predictions, test_Y)\n",
    "        accuracy_stats.append(test_accuracy)\n",
    "\n",
    "q2_stats, q2_ds = run_cv_20_iters(poly_perceptrons, poly_kernels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_params(params):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.reshape(params, (16,16)),\n",
    "                interpolation=\"None\",\n",
    "                cmap='gray',\n",
    "                vmin=0, \n",
    "                vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_params(data[6,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.genfromtxt('./zipcombo.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape\n",
    "np.unique(data1[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
